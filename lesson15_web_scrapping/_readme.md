# –õ–µ–∫—Ü–∏—è: Web Scraping —Å Beautiful Soup

## 1. –í–≤–µ–¥–µ–Ω–∏–µ –≤ Web Scraping

### 1.1 –ß—Ç–æ —Ç–∞–∫–æ–µ Web Scraping?

**Web Scraping (–ø–∞—Ä—Å–∏–Ω–≥ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü)** ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –≤–µ–±-—Å–∞–π—Ç–æ–≤.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –í–µ–±-—Å–∞–π—Ç   ‚îÇ
‚îÇ   (HTML)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ü–∞—Ä—Å–µ—Ä     ‚îÇ
‚îÇ (Beautiful  ‚îÇ
‚îÇ   Soup)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∏-  ‚îÇ
‚îÇ —Ä–æ–≤–∞–Ω–Ω—ã–µ    ‚îÇ
‚îÇ   –¥–∞–Ω–Ω—ã–µ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 –î–ª—è —á–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è?

#### üìä –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–Ω –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
- –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞
- –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤

#### üì∞ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
- –°–±–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–π —Å —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π

#### üîç SEO –∏ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥
- –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –±—Ä–µ–Ω–¥–∞
- –°–±–æ—Ä –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤

#### üìà –§–∏–Ω–∞–Ω—Å—ã
- –°–±–æ—Ä –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∞–∫—Ü–∏–π
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
- –ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π

#### üéì –ù–∞—É—á–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
- –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
- –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è ML
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—É–±–ª–∏–∫–∞—Ü–∏–π

### 1.3 Beautiful Soup ‚Äî —á—Ç–æ —ç—Ç–æ?

**Beautiful Soup** ‚Äî Python –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML –∏ XML –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω–æ–≥–æ HTML
- ‚úÖ –ú–æ—â–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ–∏—Å–∫–∞
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
- ‚úÖ –•–æ—Ä–æ—à–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:**
- `lxml` ‚Äî –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Å–ª–æ–∂–Ω–µ–µ
- `Scrapy` ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
- `Selenium` ‚Äî –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤ (JavaScript)

---

## 2. –û—Å–Ω–æ–≤—ã HTML

### 2.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML –¥–æ–∫—É–º–µ–Ω—Ç–∞

```html
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <title>–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã</title>
</head>
<body>
    <header>
        <h1>–ì–ª–∞–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫</h1>
        <nav>
            <a href="/about">–û –Ω–∞—Å</a>
            <a href="/contact">–ö–æ–Ω—Ç–∞–∫—Ç—ã</a>
        </nav>
    </header>
    
    <main>
        <article class="post">
            <h2 id="post-title">–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏</h2>
            <p class="description">–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ</p>
            <div class="content">
                <p>–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...</p>
            </div>
        </article>
    </main>
    
    <footer>
        <p>&copy; 2024 –ú–æ–π —Å–∞–π—Ç</p>
    </footer>
</body>
</html>
```

### 2.2 –ö–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã HTML

#### –¢–µ–≥–∏
**–¢–µ–≥** ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π –±–ª–æ–∫ HTML.

```html
<!-- –û—Ç–∫—Ä—ã–≤–∞—é—â–∏–π –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π —Ç–µ–≥–∏ -->
<p>–≠—Ç–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>

<!-- –°–∞–º–æ–∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π—Å—è —Ç–µ–≥ -->
<img src="image.jpg" />

<!-- –í–ª–æ–∂–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ -->
<div>
    <p>–¢–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ div</p>
</div>
```

#### –ê—Ç—Ä–∏–±—É—Ç—ã
**–ê—Ç—Ä–∏–±—É—Ç—ã** ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —ç–ª–µ–º–µ–Ω—Ç–µ.

```html
<!-- –ê—Ç—Ä–∏–±—É—Ç—ã —ç–ª–µ–º–µ–Ω—Ç–∞ -->
<a href="https://example.com" 
   class="link" 
   id="main-link" 
   target="_blank">
   –°—Å—ã–ª–∫–∞
</a>

<!-- –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã -->
id        <!-- –£–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä -->
class     <!-- CSS –∫–ª–∞—Å—Å (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ) -->
href      <!-- URL –¥–ª—è —Å—Å—ã–ª–æ–∫ -->
src       <!-- –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/—Å–∫—Ä–∏–ø—Ç–æ–≤ -->
alt       <!-- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç -->
title     <!-- –í—Å–ø–ª—ã–≤–∞—é—â–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞ -->
data-*    <!-- –ö–∞—Å—Ç–æ–º–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã -->
```

#### –ö–ª–∞—Å—Å—ã –∏ ID

```html
<!-- ID - —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä -->
<div id="header">–®–∞–ø–∫–∞ —Å–∞–π—Ç–∞</div>

<!-- Class - –º–æ–∂–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—Ç—å—Å—è -->
<p class="text">–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
<p class="text highlight">–í—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
<p class="text">–¢—Ä–µ—Ç–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>

<!-- –ù–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–æ–≤ -->
<button class="btn btn-primary btn-large">–ö–Ω–æ–ø–∫–∞</button>
```

### 2.3 –ß–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ–≥–∏

```html
<!-- –ó–∞–≥–æ–ª–æ–≤–∫–∏ (h1 - —Å–∞–º—ã–π –≤–∞–∂–Ω—ã–π, h6 - –Ω–∞–∏–º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–π) -->
<h1>–ì–ª–∞–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫</h1>
<h2>–ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫</h2>

<!-- –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —Ç–µ–∫—Å—Ç -->
<p>–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç</p>
<strong>–ñ–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç</strong>
<em>–ö—É—Ä—Å–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç</em>
<span>–ò–Ω–ª–∞–π–Ω —ç–ª–µ–º–µ–Ω—Ç</span>

<!-- –°—Å—ã–ª–∫–∏ -->
<a href="https://example.com">–°—Å—ã–ª–∫–∞</a>

<!-- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è -->
<img src="photo.jpg" alt="–û–ø–∏—Å–∞–Ω–∏–µ">

<!-- –°–ø–∏—Å–∫–∏ -->
<ul>  <!-- –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ -->
    <li>–ü—É–Ω–∫—Ç 1</li>
    <li>–ü—É–Ω–∫—Ç 2</li>
</ul>

<ol>  <!-- –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ -->
    <li>–ü–µ—Ä–≤—ã–π</li>
    <li>–í—Ç–æ—Ä–æ–π</li>
</ol>

<!-- –¢–∞–±–ª–∏—Ü—ã -->
<table>
    <thead>
        <tr>
            <th>–ó–∞–≥–æ–ª–æ–≤–æ–∫ 1</th>
            <th>–ó–∞–≥–æ–ª–æ–≤–æ–∫ 2</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>–Ø—á–µ–π–∫–∞ 1</td>
            <td>–Ø—á–µ–π–∫–∞ 2</td>
        </tr>
    </tbody>
</table>

<!-- –ë–ª–æ—á–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã -->
<div>–ë–ª–æ—á–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç</div>
<section>–°–µ–∫—Ü–∏—è</section>
<article>–°—Ç–∞—Ç—å—è</article>
<header>–®–∞–ø–∫–∞</header>
<footer>–ü–æ–¥–≤–∞–ª</footer>
<nav>–ù–∞–≤–∏–≥–∞—Ü–∏—è</nav>
```

### 2.4 –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Å–ø–µ–∫—Ü–∏–∏ HTML

#### Chrome DevTools

```
1. –û—Ç–∫—Ä—ã—Ç—å DevTools:
   - Windows/Linux: F12 –∏–ª–∏ Ctrl+Shift+I
   - Mac: Cmd+Option+I

2. –í–∫–ª–∞–¥–∫–∞ Elements:
   - –ü—Ä–æ—Å–º–æ—Ç—Ä HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
   - –í—ã–¥–µ–ª–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
   - –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HTML –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

3. –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤—ã–±–æ—Ä–∞ —ç–ª–µ–º–µ–Ω—Ç–∞:
   - –ò–∫–æ–Ω–∫–∞ –∫—É—Ä—Å–æ—Ä–∞ –≤ —É–≥–ª—É
   - –ù–∞–≤–µ–¥–µ–Ω–∏–µ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Ö–æ–¥ –∫ –∫–æ–¥—É
```

#### –ü–æ–ª–µ–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

```html
<!-- –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ -->
–ü—Ä–∞–≤—ã–π –∫–ª–∏–∫ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç–µ ‚Üí Copy ‚Üí Copy selector

<!-- –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ XPath -->
–ü—Ä–∞–≤—ã–π –∫–ª–∏–∫ –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç–µ ‚Üí Copy ‚Üí Copy XPath

<!-- –ü—Ä–æ—Å–º–æ—Ç—Ä –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∏–ª–µ–π -->
–í–∫–ª–∞–¥–∫–∞ Computed –≤ –ø—Ä–∞–≤–æ–π –ø–∞–Ω–µ–ª–∏
```

---

## 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

### 3.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Beautiful Soup
pip install beautifulsoup4

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ requests –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤
pip install requests

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ lxml –ø–∞—Ä—Å–µ—Ä–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
pip install lxml

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: html5lib (–¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ HTML)
pip install html5lib
```

### 3.2 –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫

```python
# –û—Å–Ω–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
from bs4 import BeautifulSoup
import requests

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ (–ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
import json
import csv
import time
from urllib.parse import urljoin
```

### 3.3 –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–∏ Beautiful Soup
import bs4
print(f"Beautiful Soup –≤–µ—Ä—Å–∏—è: {bs4.__version__}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ requests
import requests
print(f"Requests –≤–µ—Ä—Å–∏—è: {requests.__version__}")
```

---

## 4. –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã

### 4.1 –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

```python
import requests
from bs4 import BeautifulSoup

# URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
url = "https://example.com"

# –û—Ç–ø—Ä–∞–≤–∫–∞ GET –∑–∞–ø—Ä–æ—Å–∞
response = requests.get(url)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
if response.status_code == 200:
    print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    html_content = response.text
else:
    print(f"‚ùå –û—à–∏–±–∫–∞: {response.status_code}")
```

### 4.2 –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

```python
import requests
from bs4 import BeautifulSoup

def get_page(url):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # –í—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è 4xx/5xx
        return response.text
    except requests.exceptions.Timeout:
        print("‚è±Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
    except requests.exceptions.ConnectionError:
        print("üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
    except requests.exceptions.HTTPError as e:
        print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {e}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
    return None

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
html = get_page("https://example.com")
if html:
    soup = BeautifulSoup(html, 'lxml')
```

### 4.3 –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞

```python
# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ User-Agent (–≤–∞–∂–Ω–æ!)
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

response = requests.get(url, headers=headers)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
headers = {
    'User-Agent': 'Mozilla/5.0 ...',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
    'Referer': 'https://google.com'
}
```

### 4.4 –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ BeautifulSoup

```python
from bs4 import BeautifulSoup

# HTML –∫–æ–¥
html_doc = """
<html>
<head><title>–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞</title></head>
<body>
    <p class="title"><b>–ó–∞–≥–æ–ª–æ–≤–æ–∫</b></p>
    <p class="story">–¢–µ–∫—Å—Ç –∏—Å—Ç–æ—Ä–∏–∏...</p>
</body>
</html>
"""

# –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ BeautifulSoup
soup = BeautifulSoup(html_doc, 'lxml')

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã:
# soup = BeautifulSoup(html_doc, 'html.parser')  # –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π
# soup = BeautifulSoup(html_doc, 'html5lib')     # –°–∞–º—ã–π –º—è–≥–∫–∏–π

# –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ HTML
print(soup.prettify())
```

---

## 5. –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ HTML –¥–µ—Ä–µ–≤—É

### 5.1 –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–≥–∞–º

```python
from bs4 import BeautifulSoup

html = """
<html>
<body>
    <h1>–ì–ª–∞–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫</h1>
    <p>–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <p>–í—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <div>
        <p>–í–ª–æ–∂–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    </div>
</body>
</html>
"""

soup = BeautifulSoup(html, 'lxml')

# –ù–∞–π—Ç–∏ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
h1 = soup.find('h1')
print(h1)  # <h1>–ì–ª–∞–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫</h1>
print(h1.text)  # –ì–ª–∞–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫

# –ù–∞–π—Ç–∏ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã
paragraphs = soup.find_all('p')
print(f"–ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(paragraphs)}")

for p in paragraphs:
    print(p.text)
```

### 5.2 –ü–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º

```python
html = """
<div class="container">
    <p class="text">–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç</p>
    <p class="text highlight">–í—ã–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç</p>
    <p class="description">–û–ø–∏—Å–∞–Ω–∏–µ</p>
</div>
"""

soup = BeautifulSoup(html, 'lxml')

# –ù–∞–π—Ç–∏ –ø–æ –æ–¥–Ω–æ–º—É –∫–ª–∞—Å—Å—É
text_elements = soup.find_all(class_='text')
print(f"–≠–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–ª–∞—Å—Å–æ–º 'text': {len(text_elements)}")

# –ù–∞–π—Ç–∏ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª–∞—Å—Å–∞–º (–≤—Å–µ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å)
highlighted = soup.find_all(class_='text highlight')

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
text_elements = soup.find_all('p', class_='text')

# –ù–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –ª—é–±—ã–º –∏–∑ –∫–ª–∞—Å—Å–æ–≤
elements = soup.find_all('p', class_=['text', 'description'])
```

### 5.3 –ü–æ–∏—Å–∫ –ø–æ ID

```python
html = """
<div id="header">–®–∞–ø–∫–∞</div>
<div id="content">–ö–æ–Ω—Ç–µ–Ω—Ç</div>
<div id="footer">–ü–æ–¥–≤–∞–ª</div>
"""

soup = BeautifulSoup(html, 'lxml')

# –ü–æ–∏—Å–∫ –ø–æ ID
header = soup.find(id='header')
print(header.text)  # –®–∞–ø–∫–∞

# –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
header = soup.find('div', id='header')

# ID —É–Ω–∏–∫–∞–ª–µ–Ω, –ø–æ—ç—Ç–æ–º—É find() –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
```

### 5.4 –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤

```python
html = """
<a href="https://example.com" class="link" id="main-link" title="–ü—Ä–∏–º–µ—Ä">
    –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç
</a>
<img src="photo.jpg" alt="–§–æ—Ç–æ" width="800" height="600">
"""

soup = BeautifulSoup(html, 'lxml')

link = soup.find('a')

# –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
print(link.text)  # –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç
print(link.get_text())  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞

# –ü–æ–ª—É—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
print(link['href'])  # https://example.com
print(link.get('href'))  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–±
print(link.get('data-id', '–ù–µ –Ω–∞–π–¥–µ–Ω'))  # –° –∑–Ω–∞—á–µ–Ω–∏–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

# –í—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã
print(link.attrs)  
# {'href': 'https://example.com', 'class': ['link'], 'id': 'main-link', 'title': '–ü—Ä–∏–º–µ—Ä'}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞
if link.has_attr('href'):
    print("–°—Å—ã–ª–∫–∞ –∏–º–µ–µ—Ç href")

# –†–∞–±–æ—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
img = soup.find('img')
print(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img['src']}")
print(f"–†–∞–∑–º–µ—Ä: {img['width']}x{img['height']}")
```

### 5.5 –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –¥–µ—Ä–µ–≤—É

#### –†–æ–¥–∏—Ç–µ–ª–∏, –¥–µ—Ç–∏ –∏ —Å–æ—Å–µ–¥–∏

```python
html = """
<div id="parent">
    <h1>–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
    <p id="first">–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <p id="second">–í—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <ul>
        <li>–≠–ª–µ–º–µ–Ω—Ç 1</li>
        <li>–≠–ª–µ–º–µ–Ω—Ç 2</li>
    </ul>
</div>
"""

soup = BeautifulSoup(html, 'lxml')

# –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞
first_p = soup.find('p', id='first')

# –†–æ–¥–∏—Ç–µ–ª—å
parent = first_p.parent
print(f"–†–æ–¥–∏—Ç–µ–ª—å: {parent.name}")  # div

# –í—Å–µ —Ä–æ–¥–∏—Ç–µ–ª–∏ (–¥–æ –∫–æ—Ä–Ω—è)
for parent in first_p.parents:
    print(parent.name)  # div, body, html, [document]

# –î–µ—Ç–∏
div = soup.find('div', id='parent')
children = list(div.children)
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–π: {len(children)}")

# –ü—Ä—è–º—ã–µ –¥–µ—Ç–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ–≥–∏, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞)
for child in div.children:
    if child.name:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã
        print(child.name)

# –í—Å–µ –ø–æ—Ç–æ–º–∫–∏ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)
descendants = list(div.descendants)
print(f"–í—Å–µ–≥–æ –ø–æ—Ç–æ–º–∫–æ–≤: {len(descendants)}")

# –°–ª–µ–¥—É—é—â–∏–π —Å–æ—Å–µ–¥
next_sibling = first_p.next_sibling
while next_sibling and not next_sibling.name:
    next_sibling = next_sibling.next_sibling
print(f"–°–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç: {next_sibling.text}")

# –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–æ—Å–µ–¥
previous = first_p.previous_sibling

# –í—Å–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ—Å–µ–¥–∏
for sibling in first_p.next_siblings:
    if sibling.name:
        print(sibling.name)

# –í—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ—Å–µ–¥–∏
for sibling in first_p.previous_siblings:
    if sibling.name:
        print(sibling.name)
```

### 5.6 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫

```python
# –ü–æ–∏—Å–∫ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏
results = soup.find_all('p', class_='text', id='intro')

# –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
first_three = soup.find_all('p', limit=3)

# –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)
direct_children = soup.find_all('p', recursive=False)

# –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Ç–µ–∫—Å—Ç–∞
elements = soup.find_all(string='–ò—Å–∫–æ–º—ã–π —Ç–µ–∫—Å—Ç')

# –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º—É –≤—ã—Ä–∞–∂–µ–Ω–∏—é
import re
elements = soup.find_all(string=re.compile('–ø–∞—Ç—Ç–µ—Ä–Ω'))

# –ü–æ–∏—Å–∫ –ø–æ —Ñ—É–Ω–∫—Ü–∏–∏
def has_href(tag):
    return tag.has_attr('href')

links = soup.find_all(has_href)

# –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
def custom_filter(tag):
    return (
        tag.name == 'p' and
        tag.has_attr('class') and
        'highlight' in tag['class']
    )

highlighted = soup.find_all(custom_filter)
```

---

## 6. CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã

### 6.1 –ú–µ—Ç–æ–¥ select()

**CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã** ‚Äî –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.

```python
html = """
<div class="container">
    <h1 id="title">–ó–∞–≥–æ–ª–æ–≤–æ–∫</h1>
    <p class="text">–ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <p class="text highlight">–í—Ç–æ—Ä–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ</p>
    <ul class="list">
        <li>–≠–ª–µ–º–µ–Ω—Ç 1</li>
        <li class="active">–≠–ª–µ–º–µ–Ω—Ç 2</li>
        <li>–≠–ª–µ–º–µ–Ω—Ç 3</li>
    </ul>
    <a href="/page1">–°—Å—ã–ª–∫–∞ 1</a>
    <a href="/page2" class="external">–°—Å—ã–ª–∫–∞ 2</a>
</div>
"""

soup = BeautifulSoup(html, 'lxml')

# –ü–æ —Ç–µ–≥—É
paragraphs = soup.select('p')

# –ü–æ –∫–ª–∞—Å—Å—É
texts = soup.select('.text')

# –ü–æ ID
title = soup.select('#title')

# –ö–æ–º–±–∏–Ω–∞—Ü–∏—è
highlighted = soup.select('p.text.highlight')

# –ü–æ—Ç–æ–º–∫–∏
items = soup.select('div.container li')

# –ü—Ä—è–º—ã–µ –¥–µ—Ç–∏
direct_children = soup.select('div.container > p')

# –ê—Ç—Ä–∏–±—É—Ç—ã
external_links = soup.select('a[class="external"]')
all_links = soup.select('a[href]')
specific_links = soup.select('a[href="/page1"]')

# –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å
starts_with = soup.select('a[href^="/page"]')

# –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞
ends_with = soup.select('a[href$=".pdf"]')

# –°–æ–¥–µ—Ä–∂–∏—Ç
contains = soup.select('a[href*="page"]')

# –ü—Å–µ–≤–¥–æ–∫–ª–∞—Å—Å—ã
first_item = soup.select('li:first-child')
last_item = soup.select('li:last-child')
nth_item = soup.select('li:nth-child(2)')

# –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ (–ò–õ–ò)
elements = soup.select('p, li, a')
```

### 6.2 –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–∂–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤

```python
# –ü—Ä–∏–º–µ—Ä —Å–ª–æ–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
html = """
<article class="post" data-id="123">
    <header>
        <h2 class="post-title">–ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏</h2>
        <div class="meta">
            <span class="author">–ê–≤—Ç–æ—Ä: –ò–≤–∞–Ω</span>
            <span class="date">2024-01-15</span>
        </div>
    </header>
    <div class="content">
        <p>–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏...</p>
        <a href="/read-more" class="btn">–ß–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ</a>
    </div>
</article>
"""

soup = BeautifulSoup(html, 'lxml')

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
title = soup.select_one('article.post h2.post-title').text

# –ê–≤—Ç–æ—Ä (–≤–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫)
author = soup.select_one('.meta .author').text

# –ö–Ω–æ–ø–∫–∞ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
button = soup.select_one('.content a.btn')['href']

# –ê—Ç—Ä–∏–±—É—Ç data
post_id = soup.select_one('article[data-id]')['data-id']

# –ö–æ–º–±–∏–Ω–∞—Ü–∏—è —É—Å–ª–æ–≤–∏–π
link = soup.select_one('article.post .content a[href^="/"]')
```

---

## 7. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### 7.1 –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞

```python
import requests
from bs4 import BeautifulSoup

def parse_news():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    url = "https://example-news.com"
    
    try:
        response = requests.get(url, headers={
            'User-Agent': 'Mozilla/5.0'
        })
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        articles = soup.find_all('article', class_='news-item')
        
        news_list = []
        for article in articles:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = article.find('h2', class_='title')
            title_text = title.text.strip() if title else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
            
            # –°—Å—ã–ª–∫–∞
            link = article.find('a', class_='read-more')
            link_href = link['href'] if link else None
            
            # –î–∞—Ç–∞
            date = article.find('span', class_='date')
            date_text = date.text.strip() if date else None
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            desc = article.find('p', class_='description')
            desc_text = desc.text.strip() if desc else None
            
            news_list.append({
                'title': title_text,
                'link': link_href,
                'date': date_text,
                'description': desc_text
            })
        
        return news_list
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return []

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
news = parse_news()
for item in news:
    print(f"üì∞ {item['title']}")
    print(f"   {item['date']}")
    print(f"   {item['link']}\n")
```

### 7.2 –ü–∞—Ä—Å–∏–Ω–≥ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞

```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def parse_products(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞"""
    response = requests.get(url, headers={
        'User-Agent': 'Mozilla/5.0'
    })
    
    soup = BeautifulSoup(response.text, 'lxml')
    
    products = []
    
    # –ü–æ–∏—Å–∫ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤
    items = soup.select('.product-card')
    
    for item in items:
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        name = item.select_one('.product-name')
        name_text = name.text.strip() if name else None
        
        # –¶–µ–Ω–∞
        price = item.select_one('.price')
        price_text = price.text.strip() if price else None
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—ã –≤ —á–∏—Å–ª–æ
        if price_text:
            price_value = float(
                price_text.replace('‚ÇΩ', '')
                          .replace(' ', '')
                          .replace(',', '.')
            )
        else:
            price_value = None
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img = item.select_one('.product-image img')
        img_url = urljoin(url, img['src']) if img else None
        
        # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
        link = item.select_one('a.product-link')
        product_url = urljoin(url, link['href']) if link else None
        
        # –†–µ–π—Ç–∏–Ω–≥
        rating = item.select_one('.rating')
        rating_value = float(rating['data-rating']) if rating and rating.has_attr('data-rating') else None
        
        # –ù–∞–ª–∏—á–∏–µ
        in_stock = item.select_one('.in-stock') is not None
        
        products.append({
            'name': name_text,
            'price': price_value,
            'image': img_url,
            'url': product_url,
            'rating': rating_value,
            'in_stock': in_stock
        })
    
    return products

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
products = parse_products('https://shop.example.com/products')

for product in products:
    print(f"üõçÔ∏è {product['name']}")
    print(f"   –¶–µ–Ω–∞: {product['price']} ‚ÇΩ")
    print(f"   –†–µ–π—Ç–∏–Ω–≥: {product['rating']}")
    print(f"   {'‚úÖ –í –Ω–∞–ª–∏—á–∏–∏' if product['in_stock'] else '‚ùå –ù–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏'}\n")
```

### 7.3 –ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü

```python
def parse_table(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ HTML —Ç–∞–±–ª–∏—Ü—ã"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    
    # –ù–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É
    table = soup.find('table', class_='data-table')
    
    if not table:
        return []
    
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏
    headers = []
    thead = table.find('thead')
    if thead:
        for th in thead.find_all('th'):
            headers.append(th.text.strip())
    
    # –î–∞–Ω–Ω—ã–µ
    data = []
    tbody = table.find('tbody')
    if tbody:
        for tr in tbody.find_all('tr'):
            row = {}
            cells = tr.find_all('td')
            
            for i, cell in enumerate(cells):
                header = headers[i] if i < len(headers) else f'column_{i}'
                row[header] = cell.text.strip()
            
            data.append(row)
    
    return data

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
table_data = parse_table('https://example.com/table-page')

for row in table_data:
    print(row)
```

### 7.4 –ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π

```python
import time

def parse_all_pages(base_url):
    """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    all_items = []
    page = 1
    
    while True:
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")
        
        url = f"{base_url}?page={page}"
        response = requests.get(url, headers={
            'User-Agent': 'Mozilla/5.0'
        })
        
        soup = BeautifulSoup(response.text, 'lxml')
        
        # –ù–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        items = soup.find_all('div', class_='item')
        
        if not items:
            print("–ë–æ–ª—å—à–µ –Ω–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü")
            break
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        for item in items:
            title = item.find('h3')
            if title:
                all_items.append(title.text.strip())
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_button = soup.find('a', class_='next-page')
        if not next_button:
            break
        
        page += 1
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(1)
    
    return all_items

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
items = parse_all_pages('https://example.com/catalog')
print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
```

---

## 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

### 8.1 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV

```python
import csv
from bs4 import BeautifulSoup
import requests


def save_to_csv(data, filename='output.csv'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
    if not data:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    headers = data[0].keys()
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(data)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
products = [
    {'name': '–¢–æ–≤–∞—Ä 1', 'price': 1000, 'rating': 4.5},
    {'name': '–¢–æ–≤–∞—Ä 2', 'price': 2000, 'rating': 4.8},
    {'name': '–¢–æ–≤–∞—Ä 3', 'price': 1500, 'rating': 4.2}
]

save_to_csv(products, 'products.csv')
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± —Å pandas:**

```python
import pandas as pd

def save_to_csv_pandas(data, filename='output.csv'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é pandas"""
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8')
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
save_to_csv_pandas(products, 'products_pandas.csv')
```

### 8.2 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON

```python
import json


def save_to_json(data, filename='output.json'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
news_data = [
    {
        'title': '–ù–æ–≤–æ—Å—Ç—å 1',
        'date': '2024-01-15',
        'link': 'https://example.com/news/1'
    },
    {
        'title': '–ù–æ–≤–æ—Å—Ç—å 2',
        'date': '2024-01-16',
        'link': 'https://example.com/news/2'
    }
]

save_to_json(news_data, 'news.json')
```

**–ß—Ç–µ–Ω–∏–µ –∏–∑ JSON:**

```python
def load_from_json(filename='output.json'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON"""
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
loaded_data = load_from_json('news.json')
print(loaded_data)
```

### 8.3 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel

```python
import pandas as pd


def save_to_excel(data, filename='output.xlsx'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Excel"""
    df = pd.DataFrame(data)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ Excel —Ñ–∞–π–ª–∞ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='–î–∞–Ω–Ω—ã–µ', index=False)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        workbook = writer.book
        worksheet = writer.sheets['–î–∞–Ω–Ω—ã–µ']
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —à–∏—Ä–∏–Ω–∞ —Å—Ç–æ–ª–±—Ü–æ–≤
        for i, col in enumerate(df.columns):
            max_length = max(
                df[col].astype(str).apply(len).max(),
                len(col)
            ) + 2
            worksheet.set_column(i, i, max_length)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
save_to_excel(products, 'products.xlsx')
```

### 8.4 –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite

```python
import sqlite3


def save_to_database(data, db_name='scraping.db', table_name='products'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ SQLite"""
    conn = sqlite3.connect(db_name)
    cursor = conn.cursor()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    cursor.execute(f'''
        CREATE TABLE IF NOT EXISTS {table_name} (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT,
            price REAL,
            rating REAL,
            url TEXT,
            parsed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # –í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    for item in data:
        cursor.execute(f'''
            INSERT INTO {table_name} (name, price, rating, url)
            VALUES (?, ?, ?, ?)
        ''', (
            item.get('name'),
            item.get('price'),
            item.get('rating'),
            item.get('url')
        ))
    
    conn.commit()
    conn.close()
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {db_name}")


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
save_to_database(products)
```

---

## 9. –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### 9.1 –û–±—Ä–∞–±–æ—Ç–∫–∞ JavaScript-—Å–∞–π—Ç–æ–≤

–î–ª—è —Å–∞–π—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö JavaScript, Beautiful Soup –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ **Selenium**.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞
options = webdriver.ChromeOptions()
options.add_argument('--headless')  # –ë–µ–∑ GUI
driver = webdriver.Chrome(options=options)

try:
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    driver.get('https://dynamic-site.com')
    
    # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, 'product-card'))
    )
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ HTML –ø–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è JavaScript
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∫ –æ–±—ã—á–Ω–æ
    products = soup.find_all('div', class_='product-card')
    
finally:
    driver.quit()
```

### 9.2 –†–∞–±–æ—Ç–∞ —Å —Ñ–æ—Ä–º–∞–º–∏ –∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π

```python
import requests
from bs4 import BeautifulSoup

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏
session = requests.Session()

# –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
login_url = 'https://example.com/login'
login_data = {
    'username': 'user@example.com',
    'password': 'password123'
}

response = session.post(login_url, data=login_data)

if response.ok:
    # –¢–µ–ø–µ—Ä—å –º–æ–∂–µ–º –¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å—ã –æ—Ç –∏–º–µ–Ω–∏ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    protected_page = session.get('https://example.com/profile')
    soup = BeautifulSoup(protected_page.text, 'lxml')
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—â–∏—â–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    user_data = soup.find('div', class_='user-info')
    print(user_data.text)
```

### 9.3 –û–±—Ä–∞–±–æ—Ç–∫–∞ AJAX –∑–∞–ø—Ä–æ—Å–æ–≤

```python
import requests
import json

# –ú–Ω–æ–≥–∏–µ —Å–∞–π—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ AJAX
# –ù–∞–π–¥–∏—Ç–µ –Ω—É–∂–Ω—ã–π endpoint –≤ DevTools ‚Üí Network ‚Üí XHR

def parse_ajax_data(url, params=None):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ AJAX –∑–∞–ø—Ä–æ—Å–∞"""
    headers = {
        'User-Agent': 'Mozilla/5.0',
        'X-Requested-With': 'XMLHttpRequest',  # –í–∞–∂–Ω–æ –¥–ª—è AJAX
        'Accept': 'application/json'
    }
    
    response = requests.get(url, headers=headers, params=params)
    
    if response.ok:
        data = response.json()
        return data
    
    return None

# –ü—Ä–∏–º–µ—Ä
ajax_url = 'https://example.com/api/products'
params = {'page': 1, 'limit': 20}

products = parse_ajax_data(ajax_url, params)
if products:
    for product in products['items']:
        print(product['name'])
```

### 9.4 –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö

```python
import requests
from bs4 import BeautifulSoup
import time
from concurrent.futures import ThreadPoolExecutor


def parse_page(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'lxml')
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        items = soup.find_all('div', class_='item')
        return [item.text.strip() for item in items]
    
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
        return []


def parse_multiple_pages(urls, max_workers=5):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    all_results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
        results = executor.map(parse_page, urls)
        
        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        for page_results in results:
            all_results.extend(page_results)
    
    return all_results


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
urls = [f'https://example.com/page/{i}' for i in range(1, 11)]
results = parse_multiple_pages(urls, max_workers=5)

print(f"–°–æ–±—Ä–∞–Ω–æ {len(results)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
```

### 9.5 –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–æ–∫

```python
import requests
from bs4 import BeautifulSoup

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
response = requests.get(url)
response.encoding = response.apparent_encoding  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
soup = BeautifulSoup(response.text, 'lxml')

# –Ø–≤–Ω–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
response = requests.get(url)
soup = BeautifulSoup(response.content, 'lxml', from_encoding='windows-1251')

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    text = element.text
except UnicodeDecodeError:
    text = element.text.encode('utf-8', errors='ignore').decode('utf-8')
```

---

## 10. –≠—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã Web Scraping

### 10.1 –õ–µ–≥–∞–ª—å–Ω–æ—Å—Ç—å –∏ robots.txt

**robots.txt** ‚Äî —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤.

```python
import requests
from urllib.robotparser import RobotFileParser

def can_fetch(url):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Ä–∞–∑—Ä–µ—à—ë–Ω –ª–∏ –ø–∞—Ä—Å–∏–Ω–≥"""
    parser = RobotFileParser()
    robots_url = f"{url.split('/')[0]}//{url.split('/')[2]}/robots.txt"
    parser.set_url(robots_url)
    parser.read()
    
    user_agent = 'MyBot'
    return parser.can_fetch(user_agent, url)

# –ü—Ä–æ–≤–µ—Ä–∫–∞
url = 'https://example.com/products'
if can_fetch(url):
    print("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑—Ä–µ—à—ë–Ω")
else:
    print("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø—Ä–µ—â—ë–Ω")
```

**–ü—Ä–∏–º–µ—Ä robots.txt:**

```
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/

Crawl-delay: 10
```

### 10.2 –ü—Ä–∞–≤–∏–ª–∞ —Ö–æ—Ä–æ—à–µ–≥–æ —Ç–æ–Ω–∞

#### ‚úÖ DO (–î–µ–ª–∞–π—Ç–µ)

```python
# 1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
import time

for url in urls:
    response = requests.get(url)
    # ... –ø–∞—Ä—Å–∏–Ω–≥ ...
    time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ 1 —Å–µ–∫—É–Ω–¥–∞

# 2. –£–∫–∞–∑—ã–≤–∞–π—Ç–µ User-Agent
headers = {
    'User-Agent': 'MyBot/1.0 (+https://mysite.com/bot-info)'
}

# 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –æ—à–∏–±–∫–∏
try:
    response = requests.get(url, timeout=10)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"–û—à–∏–±–∫–∞: {e}")

# 4. –ö–µ—à–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
import hashlib
import pickle

def get_cached_or_fetch(url, cache_dir='cache'):
    cache_key = hashlib.md5(url.encode()).hexdigest()
    cache_file = f"{cache_dir}/{cache_key}.pkl"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
    try:
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    except FileNotFoundError:
        pass
    
    # –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö
    response = requests.get(url)
    data = response.text
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à
    with open(cache_file, 'wb') as f:
        pickle.dump(data, f)
    
    return data

# 5. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Ç–µ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=3) as executor:  # –ù–µ –±–æ–ª–µ–µ 3 –ø–æ—Ç–æ–∫–æ–≤
    results = executor.map(parse_page, urls)
```

#### ‚ùå DON'T (–ù–µ –¥–µ–ª–∞–π—Ç–µ)

```python
# ‚ùå –ù–µ –¥–µ–ª–∞–π—Ç–µ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã
for i in range(10000):
    requests.get(url)  # DDoS!

# ‚ùå –ù–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–π—Ç–µ robots.txt
# –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ robots.txt –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º

# ‚ùå –ù–µ —Å–∫—Ä—ã–≤–∞–π—Ç–µ —Å–≤–æ—ë –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ
headers = {'User-Agent': 'Mozilla/5.0...'}  # –í—ã–¥–∞—ë–º —Å–µ–±—è –∑–∞ –±—Ä–∞—É–∑–µ—Ä

# ‚ùå –ù–µ –ø–∞—Ä—Å–∏—Ç–µ –ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Å–æ–≥–ª–∞—Å–∏—è
# –£–≤–∞–∂–∞–π—Ç–µ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

# ‚ùå –ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π—Ç–µ —Å–µ—Ä–≤–µ—Ä
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–∞–∑—É–º–Ω—ã–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
```

### 10.3 Terms of Service (ToS)

**–í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ Terms of Service —Å–∞–π—Ç–∞:**

- üìú –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∞–π—Ç—ã —è–≤–Ω–æ –∑–∞–ø—Ä–µ—â–∞—é—Ç –ø–∞—Ä—Å–∏–Ω–≥
- ‚öñÔ∏è –ù–∞—Ä—É—à–µ–Ω–∏–µ ToS –º–æ–∂–µ—Ç –∏–º–µ—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è
- ü§ù –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ API

### 10.4 –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

```python
import time
import random
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry


class EthicalScraper:
    """–≠—Ç–∏—á–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
    
    def __init__(self, delay_range=(1, 3), max_retries=3):
        self.delay_range = delay_range
        self.session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def get(self, url, **kwargs):
        """GET –∑–∞–ø—Ä–æ—Å —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
        
        # –ó–∞–ø—Ä–æ—Å
        headers = kwargs.get('headers', {})
        headers.setdefault('User-Agent', 'EthicalScraper/1.0')
        kwargs['headers'] = headers
        
        try:
            response = self.session.get(url, timeout=10, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
            return None


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
scraper = EthicalScraper(delay_range=(2, 5))

for url in urls:
    response = scraper.get(url)
    if response:
        soup = BeautifulSoup(response.text, 'lxml')
        # ... –ø–∞—Ä—Å–∏–Ω–≥ ...
```

---

## 11. –û—Ç–ª–∞–¥–∫–∞ –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

### 11.1 –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏

#### 404 Not Found

```python
response = requests.get(url)
if response.status_code == 404:
    print("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
elif response.status_code == 200:
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    pass
```

#### –≠–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω

```python
# ‚ùå –ü–ª–æ—Ö–æ
title = soup.find('h1', class_='title').text  # AttributeError –µ—Å–ª–∏ None

# ‚úÖ –•–æ—Ä–æ—à–æ
title_element = soup.find('h1', class_='title')
title = title_element.text if title_element else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"

# ‚úÖ –ï—â—ë –ª—É—á—à–µ
title = soup.find('h1', class_='title')
if title:
    print(title.text)
else:
    print("–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω")
```

#### –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞

```python
def safe_parse(soup):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏"""
    # –ü–æ–ø—ã—Ç–∫–∞ 1
    title = soup.find('h1', class_='title')
    if title:
        return title.text
    
    # –ü–æ–ø—ã—Ç–∫–∞ 2
    title = soup.find('h2', class_='page-title')
    if title:
        return title.text
    
    # –ü–æ–ø—ã—Ç–∫–∞ 3
    title = soup.select_one('#main-title')
    if title:
        return title.text
    
    return "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
```

### 11.2 –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def parse_with_logging(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    logger.info(f"–ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {url}")
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'lxml')
        items = soup.find_all('div', class_='item')
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(items)}")
        return items
    
    except requests.exceptions.Timeout:
        logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}")
    except requests.exceptions.HTTPError as e:
        logger.error(f"HTTP –æ—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ {url}: {e}")
    
    return []
```

### 11.3 –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏

```python
# –í—ã–≤–æ–¥ –∫—Ä–∞—Å–∏–≤–æ–≥–æ HTML
print(soup.prettify())

# –í—ã–≤–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
element = soup.find('div', class_='content')
print(element.prettify())

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞
if element.has_attr('data-id'):
    print(f"ID: {element['data-id']}")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
print(element.attrs)

# –ü—É—Ç—å –¥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
def get_path(element):
    """–ü–æ–ª—É—á–∏—Ç—å CSS –ø—É—Ç—å –¥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    path = []
    while element:
        if element.name:
            selector = element.name
            if element.get('id'):
                selector += f"#{element['id']}"
            elif element.get('class'):
                selector += f".{'.'.join(element['class'])}"
            path.insert(0, selector)
        element = element.parent
    return ' > '.join(path)

element = soup.find('div', class_='content')
print(get_path(element))
```

---

## 12. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ

### –ó–∞–¥–∞–Ω–∏–µ 1: –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π

–°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞:

```python
"""
–ó–∞–¥–∞—á–∞:
1. –í—ã–±–µ—Ä–∏—Ç–µ –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å–∞–π—Ç
2. –ò–∑–≤–ª–µ–∫–∏—Ç–µ:
   - –ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
   - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
   - –°—Å—ã–ª–∫–∏ –Ω–∞ –ø–æ–ª–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
   - –î–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
3. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ CSV
4. –í—ã–≤–µ–¥–∏—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É: —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ
"""

def parse_news_site():
    # –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å
    pass

if __name__ == "__main__":
    news = parse_news_site()
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≤—ã–≤–æ–¥
```

### –ó–∞–¥–∞–Ω–∏–µ 2: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–Ω

```python
"""
–ó–∞–¥–∞—á–∞:
1. –í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω
2. –ò–∑–≤–ª–µ–∫–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–∞—Ö:
   - –ù–∞–∑–≤–∞–Ω–∏–µ
   - –¶–µ–Ω–∞
   - –ù–∞–ª–∏—á–∏–µ
   - –†–µ–π—Ç–∏–Ω–≥
3. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ JSON
4. –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω
"""

def monitor_prices(urls):
    # –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å
    pass
```

### –ó–∞–¥–∞–Ω–∏–µ 3: –ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –≤–∞–∫–∞–Ω—Å–∏–π

```python
"""
–ó–∞–¥–∞—á–∞:
1. –í—ã–±–µ—Ä–∏—Ç–µ —Å–∞–π—Ç —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
2. –ò–∑–≤–ª–µ–∫–∏—Ç–µ:
   - –ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
   - –ö–æ–º–ø–∞–Ω–∏—è
   - –ó–∞—Ä–ø–ª–∞—Ç–∞
   - –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
   - –ö–æ–Ω—Ç–∞–∫—Ç—ã
3. –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö SQLite
4. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
"""

def scrape_jobs(search_query):
    # –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å
    pass
```

---

## 13. –ü–æ–ª–µ–∑–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –∏ —Ç—Ä—é–∫–∏

### 13.1 –†–∞–±–æ—Ç–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º

```python
# –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–∫—Ä–æ–ª–ª–µ
from selenium import webdriver
import time

driver = webdriver.Chrome()
driver.get(url)

# –°–∫—Ä–æ–ª–ª –≤–Ω–∏–∑ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
last_height = driver.execute_script("return document.body.scrollHeight")

while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(2)
    
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

html = driver.page_source
soup = BeautifulSoup(html, 'lxml')
```

### 13.2 –û–±—Ö–æ–¥ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫

```python
# –†–æ—Ç–∞—Ü–∏—è User-Agent
import random

user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',
    'Mozilla/5.0 (X11; Linux x86_64)...'
]

headers = {
    'User-Agent': random.choice(user_agents)
}

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏
proxies = {
    'http': 'http://proxy.example.com:8080',
    'https': 'https://proxy.example.com:8080'
}

response = requests.get(url, headers=headers, proxies=proxies)
```

### 13.3 –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–∫—Ä–∏–ø—Ç–æ–≤

```python
import json
import re

# –ú–Ω–æ–≥–∏–µ —Å–∞–π—Ç—ã –≤—Å—Ç—Ä–∞–∏–≤–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –≤ JavaScript
html = """
<script>
var productData = {
    "name": "–¢–æ–≤–∞—Ä",
    "price": 1000,
    "rating": 4.5
};
</script>
"""

soup = BeautifulSoup(html, 'lxml')

# –ü–æ–∏—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞
script = soup.find('script', string=re.compile('productData'))
if script:
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON
    json_text = re.search(r'var productData = ({.*?});', script.string, re.DOTALL)
    if json_text:
        data = json.loads(json_text.group(1))
        print(data)
```

---

## –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

### –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

‚úÖ **Beautiful Soup** ‚Äî –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML/XML  
‚úÖ **requests** ‚Äî –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü  
‚úÖ **HTML** ‚Äî —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫ —Ä–∞–∑–º–µ—Ç–∫–∏  
‚úÖ **–°–µ–ª–µ–∫—Ç–æ—Ä—ã** ‚Äî —Ç–µ–≥–∏, –∫–ª–∞—Å—Å—ã, ID, CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä—ã  

### –ú–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞

| –ú–µ—Ç–æ–¥ | –û–ø–∏—Å–∞–Ω–∏–µ | –ü—Ä–∏–º–µ—Ä |
|-------|----------|--------|
| `find()` | –ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç | `soup.find('div')` |
| `find_all()` | –í—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã | `soup.find_all('p')` |
| `select()` | CSS —Å–µ–ª–µ–∫—Ç–æ—Ä | `soup.select('.class')` |
| `select_one()` | –ü–µ—Ä–≤—ã–π –ø–æ CSS | `soup.select_one('#id')` |

### –§–æ—Ä–º–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

- üìÑ **CSV** ‚Äî –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- üìã **JSON** ‚Äî –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- üìä **Excel** ‚Äî –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤
- üóÑÔ∏è **SQLite** ‚Äî –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤

### –≠—Ç–∏–∫–∞

‚ö†Ô∏è **–£–≤–∞–∂–∞–π—Ç–µ robots.txt**  
‚ö†Ô∏è **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–¥–µ—Ä–∂–∫–∏**  
‚ö†Ô∏è **–£–∫–∞–∑—ã–≤–∞–π—Ç–µ User-Agent**  
‚ö†Ô∏è **–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ ToS**  
‚ö†Ô∏è **–ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π—Ç–µ —Å–µ—Ä–≤–µ—Ä—ã**  

---

## –ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

üìö **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:**
- Beautiful Soup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Requests: https://requests.readthedocs.io/
- Selenium: https://selenium-python.readthedocs.io/

üìñ **–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏:**
- `beautifulsoup4` ‚Äî –ø–∞—Ä—Å–∏–Ω–≥ HTML/XML
- `requests` ‚Äî HTTP –∑–∞–ø—Ä–æ—Å—ã
- `selenium` ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞
- `scrapy` ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
- `lxml` ‚Äî –±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–µ—Ä
- `pandas` ‚Äî –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

üõ†Ô∏è **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:**
- Chrome DevTools ‚Äî –∏–Ω—Å–ø–µ–∫—Ü–∏—è HTML
- Postman ‚Äî —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API
- SelectorGadget ‚Äî –ø–æ–º–æ—â—å —Å CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏

üì∫ **–ü—Ä–∞–∫—Ç–∏–∫–∞:**
- –¢—Ä–µ–Ω–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö —Å–∞–π—Ç–∞—Ö
- –ò–∑—É—á–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
- –°–æ–∑–¥–∞–≤–∞–π—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
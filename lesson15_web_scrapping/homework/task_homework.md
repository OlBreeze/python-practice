# Технічне завдання: Парсинг новинного сайту

Вам потрібно створити Python-скрипт, який буде парсити головну сторінку новинного сайту та збирати інформацію про останні новини. Ваш скрипт повинен отримувати заголовки новин, посилання на повний текст новини, дату публікації та короткий опис (якщо він присутній на сторінці).

## Технічні вимоги

### 1. Використання бібліотек
* Використайте бібліотеки `requests` для завантаження HTML-коду сторінки та `BeautifulSoup` для парсингу HTML.
* Для додаткової обробки отриманих даних (опціонально) можна використовувати `pandas`.

### 2. Цільова сторінка
* Виберіть будь-який новинний сайт, який має простий HTML-код та не вимагає авторизації для доступу до новин.
* Наприклад, https://example.com/news (замість цього потрібно вибрати сайт, який підтримує основи HTML).

### 3. Інформація для збору
* Заголовок кожної новини.
* Посилання на сторінку з повним текстом новини.
* Дата публікації.
* Короткий опис або анотація новини (якщо є).

### 4. Реалізація функцій
* `get_page(url)`: завантажує HTML-код сторінки за вказаною URL та повертає `BeautifulSoup`-об'єкт.
* `parse_news(soup)`: отримує `soup`-об'єкт сторінки та витягує всі новини у вигляді списку словників із ключами `title`, `link`, `date`, `summary`.
* `save_to_csv(data)`: приймає список новин та зберігає його в CSV-файлі (назва файлу – `news.csv`).

### 5. Додаткові вимоги
* Використовуйте виключення для обробки помилок (наприклад, якщо сторінка не завантажується, чи HTML-елементи не знайдені).
* Переконайтеся, що всі дані зберігаються у структурованому форматі (наприклад, у CSV-файлі).

## Очікуваний результат

1. Запустіть скрипт, який завантажить сторінку новин, збере інформацію про останні новини та збереже її у форматі CSV.
2. Файл `news.csv` повинен містити стовпці `title`, `link`, `date`, `summary` з відповідними даними для кожної новини.

## Додаткові завдання

1. **Фільтрація за датою**: Додайте можливість фільтрувати новини за останні кілька днів (наприклад, новини за останні 7 днів).
2. **Візуалізація**: Використайте бібліотеку `pandas` для виведення короткого статистичного звіту (наприклад, скільки новин було опубліковано кожного дня).

## Критерії оцінювання

1. Правильне використання `BeautifulSoup` для навігації та пошуку HTML-елементів.
2. Здатність отримати необхідні дані (заголовок, дату, посилання, опис).
3. Коректне збереження інформації у CSV.
4. Відсутність помилок при виконанні коду та обробка виключень (помилок).
---
---
# Технічна реалізація

## Функціональні вимоги

### 1. Збір даних
Скрипт повинен витягувати наступну інформацію для кожної новини:
- **Заголовок** новини
- **Посилання** на повний текст новини
- **Дата публікації**
- **Короткий опис** або анотація (якщо присутня на сторінці)

### 2. Обов'язкові функції

#### `get_page(url)`
- **Вхідні параметри**: URL сторінки (string)
- **Повертає**: BeautifulSoup-об'єкт
- **Призначення**: завантажує HTML-код сторінки та створює об'єкт для парсингу

#### `parse_news(soup)`
- **Вхідні параметри**: BeautifulSoup-об'єкт
- **Повертає**: список словників з новинами
- **Структура словника**:
  ```python
  {
      'title': 'Заголовок новини',
      'link': 'https://example.com/news/article',
      'date': '2025-10-15',
      'summary': 'Короткий опис новини'
  }
  ```

#### `save_to_csv(data)`
- **Вхідні параметри**: список словників з новинами
- **Повертає**: None
- **Призначення**: зберігає дані у CSV-файл з іменем `news.csv`

---

## Технічні вимоги

### Бібліотеки
**Обов'язкові:**
- `requests` - для HTTP-запитів та завантаження HTML
- `beautifulsoup4` (BeautifulSoup) - для парсингу HTML-структури

**Опціональні:**
- `pandas` - для обробки даних та створення звітів
- `datetime` - для роботи з датами

### Цільовий сайт
- Новинний сайт з простою HTML-структурою
- Без вимог авторизації
- Приклад: потрібно обрати реальний сайт (наприклад, BBC News, The Guardian, або український новинний портал)

### Формат вихідних даних
**CSV-файл** `news.csv` з наступною структурою:

| title | link | date | summary |
|-------|------|------|---------|
| Заголовок 1 | https://... | 2025-10-15 | Опис... |
| Заголовок 2 | https://... | 2025-10-14 | Опис... |

---

## Обробка помилок

Скрипт повинен коректно обробляти наступні ситуації:
- Помилка з'єднання з сервером
- Неіснуючий URL або HTTP 404
- Відсутність необхідних HTML-елементів на сторінці
- Помилки при збереженні файлу
- Некоректний формат дати

**Приклад обробки:**
```python
try:
    # код парсингу
except requests.exceptions.RequestException as e:
    print(f"Помилка завантаження сторінки: {e}")
except Exception as e:
    print(f"Загальна помилка: {e}")
```

---

## Додаткові завдання (опціонально)

### 1. Фільтрація за датою
Реалізувати функцію `filter_by_date(news_list, days)`:
- **Параметри**: список новин, кількість днів
- **Повертає**: відфільтровані новини за останні N днів
- **Приклад**: новини за останні 7 днів

### 2. Статистичний звіт
Використовуючи `pandas`, створити звіт з інформацією:
- Загальна кількість зібраних новин
- Кількість новин за кожен день
- Середня довжина заголовків
- Топ-5 днів за кількістю публікацій

---

## Очікуваний результат

При успішному виконанні:
1. Скрипт запускається без помилок
2. Завантажується HTML-сторінка новинного сайту
3. Витягується інформація про всі новини з головної сторінки
4. Створюється файл `news.csv` зі структурованими даними
5. У консоль виводиться інформація про кількість зібраних новин

---

## Критерії оцінювання

### Обов'язкові критерії (70%)
- ✅ Коректне використання `requests` для завантаження сторінки
- ✅ Правильне використання `BeautifulSoup` для навігації по DOM
- ✅ Успішне витягування всіх необхідних даних (заголовок, посилання, дата, опис)
- ✅ Коректне збереження даних у CSV-файл
- ✅ Обробка помилок та виключень

### Додаткові критерії (30%)
- ✅ Чистий та читабельний код
- ✅ Використання функцій згідно специфікації
- ✅ Документація коду (docstrings)
- ✅ Реалізація додаткових завдань (фільтрація, візуалізація)

---

## Структура проєкту

```
news_parser/
│
├── main.py              # Основний скрипт
├── requirements.txt     # Залежності проєкту
├── news.csv            # Вихідний файл (генерується)
└── README.md           # Документація
```

### requirements.txt
```
requests>=2.31.0
beautifulsoup4>=4.12.0
pandas>=2.0.0
lxml>=4.9.0
```

---

## Приклад використання

```python
if __name__ == "__main__":
    url = "https://example-news-site.com"
    
    # Завантаження сторінки
    soup = get_page(url)
    
    # Парсинг новин
    news_data = parse_news(soup)
    
    # Збереження результатів
    save_to_csv(news_data)
    
    print(f"Зібрано {len(news_data)} новин")
```

---

## Терміни виконання
- Базова реалізація: 2-3 години
- З додатковими завданнями: 4-5 годин